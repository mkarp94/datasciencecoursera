abline(h = mu_y, v = mu_x)
# Add line segments
segments(x0 = 0, y0 = -1e10, x1 = 0, y1 = 0, col="red")
segments(x0 = -1e10, y0 = 0, x1 = 0, y1 = 0, col="red")
# Compute joint probability
pmvnorm(lower = c(-Inf, -Inf), upper = c(0, 0),
mean = c(mu_x, mu_y), sigma = Sigma_xy)
matA = matrix(c(4,9,2,1),2,2,byrow=T)
matB = matrix(c(2,0,0,7),2,2,byrow=T)
matA
matB
# matrix addition
matC = matA + matB
matC
# matrix subtraction
matC = matA - matB
matC
# scalar multiplication
matA
matC = 2*matA
matC
# matrix multiplication
matA = matrix(1:4,2,2,byrow=T)
matB = matrix(5:8,2,2,byrow=T)
matA
matB
dim(matA)
dim(matB)
matC = matA%*%matB
matC
# represent system in matrix algebra notation
matA = matrix(c(1,1,2,-1), 2, 2, byrow=TRUE)
vecB = c(1,1)
matA.inv = solve(matA)
matA.inv
matA.inv%*%matA
matA%*%matA.inv
z = matA.inv%*%vecB
z
matA
matA.inv = solve(matA)
matA.inv
# simulate Gaussian White Noise process
set.seed(123)
y = rnorm(250)
ts.plot(y,main="Gaussian White Noise Process",xlab="time",ylab="y(t)",
col="blue", lwd=2)
abline(h=0)
# equivalent plot using plot()
plot(y, main="Gaussian White Noise Process", type="l", xlab="time",ylab="y(t)",
col="blue", lwd=2)
abline(h=0)
ma1.model.5 = list(ma=0.5)
mu = 0.05
set.seed(123)
ma1.sim.5 = mu + arima.sim(model=ma1.model.5, n=250,
innov=rnorm(n=250, mean=0, sd=0.1))
sum(ma1.sim.5)
str(ma1.sim.5)
ma1.model = list(ma=0.9)
mu = 1
set.seed(123)
ma1.sim = mu + arima.sim(model=ma1.model,n=250)
# simulate MA(1) process with theta 0.9 and e(t) ~ N(0,(0.1)^2)
set.seed(123)
ma1.sim2 = mu + arima.sim(model=ma1.model, n=250, innov=rnorm(n=250, mean=0, sd=0.1))
# ACF for MA(1) model
ma1.acf = ARMAacf(ar=0, ma=0.9, lag.max=10)
ma1.acf
par(mfrow=c(2,1))
ts.plot(ma1.sim,main="MA(1) Process: mu=1, theta=0.9",
xlab="time",ylab="y(t)", col="blue", lwd=2)
abline(h=c(0,1))
set.seed(123);
# Simulate 250 observations from the described MA(1) model
mu = .05
ma1_sim = mu + arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1);
set.seed(123);
# Simulate 250 observations from the described MA(1) model
ma1_sim = arima.sim(model=list(ma=0.5), n=250, mean=0, sd=0.1) + 0.05;
# A line plot of the simulated observations
plot(ma1_sim,main="MA(1) Process: mu=0.05, theta=0.5",
xlab="time",ylab="y(t)", type = "l", col="blue", lwd=2)
abline(h=0)
# Generate the theoretical ACF with upto lag 10
acf_ma1_model = ARMAacf(ma = .5, lag.max = 10)
# Split plotting window in three rows
par(mfrow=c(3,1))
# First plot: The simulated observations
plot(ma1_sim, type="l",main="MA(1) Process: mu=0.05, theta=0.5",xlab="time",ylab="y(t)")
abline(h=0)
# Second plot: Theoretical ACF
plot(1:10, acf_ma1_model[2:11], type="h", col="blue",  ylab="ACF", main="theoretical ACF")
# Third plot: Sample ACF
tmp = acf(ma1_sim, lag.max=10, main="Sample ACF")# Assign to tmp the Sample ACF
# Reset graphical window to only one graph
par(mfrow=c(1,1))
set.seed(123);
# Simulate 250 observations from the described MA(1) model
ma1_sim = arima.sim(model=list(ma=0.9), n=250, mean=0, sd=0.1) + 0.05;
# Generate the theoretical ACF with upto lag 10
acf_ma1_model = ARMAacf(ma=0.9, lag.max=10)
# Split plotting window in three rows
par(mfrow=c(3,1))
# First plot: The simulated observations
plot(ma1_sim, type="l",main="MA(1) Process: mu=0.05, theta=0.9",xlab="time",ylab="y(t)")
abline(h=0)
# Second plot: Theoretical ACF
plot(1:10, acf_ma1_model[2:11], type="h", col="blue", main="theoretical ACF")
# Third plot: Sample ACF
tmp=acf(ma1_sim, lag.max=10, main="Sample ACF")
# Reset graphical window to only one graph
par(mfrow=c(1,1))
#tests for graphs
set.seed(123);
# Simulate 250 observations from the described AR(1) model
ar1_sim = arima.sim(model=list(ar=0.9), n=250, mean=0, sd=0.1) + 0.05;
# Generate the theoretical ACF with ten lags
acf_ar1_model = ARMAacf(ar=0.9, lag.max=10)
# Split plotting window in three rows
par(mfrow=c(3,1))
# Generate the same three graphs as in the previous exercise
par(mfrow=c(3,1))
# First plot: The simulated observations
plot(ar1_sim, type="l", main="AR(1) Process: mu=0.05, phi=0.9",xlab="time",ylab="y(t)")
abline(h=0)
# Second plot: Theoretical AFC
plot(1:10, acf_ar1_model[2:11], type="h", col="blue", main="theoretical ACF")
# Third plot: Sample AFC
tmp = acf(ar1_sim, lag.max=10, main="Sample ACF")
# Reset plotting window to default
par(mfrow=c(1,1));
install.packages(PerformanceAnalytics)
install.packages("PerformanceAnalytics")
install.packages("zoo")
install.packages("tseries")
library(PerformanceAnalytics)
library(zoo)
library(tseries)
?get.hist.quote
# getting data into time series objects from yahoo
# Load relevant packages
library(PerformanceAnalytics);library(zoo);library(tseries);
# Get the monthly adjusted closing price data on VBLTX, FMAGX and SBUX from Yahoo! using the tseries function get.hist.quote(). Set the sample to Jan 1998 through Dec 2009.
# Get the adjusted closing prices from Yahoo!
VBLTX_prices = get.hist.quote(instrument="vbltx", start="1998-01-01",end="2009-12-31", quote="AdjClose",provider="yahoo", origin="1970-01-01",compression="m", retclass="zoo")
FMAGX_prices = get.hist.quote(instrument="fmagx", start="1998-01-01",end="2009-12-31", quote="AdjClose",provider="yahoo", origin="1970-01-01",compression="m", retclass="zoo")
SBUX_prices = get.hist.quote(instrument="sbux", start="1998-01-01",end="2009-12-31", quote="AdjClose",provider="yahoo", origin="1970-01-01",compression="m", retclass="zoo")
# Change the class of the time index to yearmon which is appropriate for monthly data
# index() and as.yearmon() are functions in the zoo package
index(VBLTX_prices) = as.yearmon(index(VBLTX_prices))
index(FMAGX_prices)  = as.yearmon(index(FMAGX_prices))
index(SBUX_prices)  = as.yearmon(index(SBUX_prices))
# Inspect your data
start(SBUX_prices)
end(SBUX_prices)
# The variables VBLTX_prices, FMAGX_prices and SBUX_prices are preloaded in your workspace
# Create merged price data
all_prices = merge(VBLTX_prices, FMAGX_prices, SBUX_prices)
# Rename columns
colnames(all_prices) = c("VBLTX", "FMAGX", "SBUX")
# Calculate cc returns as difference in log prices
all_returns = diff(log(all_prices))
# Look at the return data
start(all_returns)
end(all_returns)
colnames(all_returns)
head(all_returns)
# Generate a chart based on assets in portfolio displaying cumulative return on 1$ since purchase
# 'all_returns' is preloaded in your workspace.
# Plot returns after using the PerformanceAnalytics function chart.TimeSeries().
# This function creates a slightly nicer looking plot than plot.zoo()
chart.TimeSeries(all_returns, legend.loc="bottom", main=" ")
# The previous charts are a bit hard to read. The PerformanceAnalytics function
# chart.Bar makes it easier to compare the returns of different assets on the
# same plot
chart.Bar(all_returns, legend.loc="bottom", main=" ")
# Cumulative return plot - must use simple returns (!) and not cc returns for this
# Use PerformanceAnalytics function chart.CumReturns()
simple_returns = diff(all_prices)/lag(all_prices, k=-1);
chart.CumReturns(simple_returns, wealth.index = TRUE, legend.loc = "topleft", main = "Future Value of $1 invested")
# Create matrix with returns
return_matrix = coredata(all_returns);
# Generate four panel plots
par(mfrow=c(2,2))
hist(return_matrix[,"VBLTX"],main="VBLTX monthly returns",
xlab="VBLTX", probability=T, col="slateblue1")
boxplot(return_matrix[,"VBLTX"],outchar=T, main="Boxplot", col="slateblue1")
plot(density(return_matrix[,"VBLTX"]),type="l", main="Smoothed density",
xlab="monthly return", ylab="density estimate", col="slateblue1")
qqnorm(return_matrix[,"VBLTX"], col="slateblue1")
qqline(return_matrix[,"VBLTX"])
par(mfrow=c(1,1))
chart.CumReturns(simple_returns, wealth.index = TRUE, legend.loc = "topleft", main = "Future Value of $1 invested")
# Generate four panel plots
par(mfrow=c(2,2))
hist(return_matrix[,"VBLTX"],main="VBLTX monthly returns",
xlab="VBLTX", probability=T, col="slateblue1")
boxplot(return_matrix[,"VBLTX"],outchar=T, main="Boxplot", col="slateblue1")
plot(density(return_matrix[,"VBLTX"]),type="l", main="Smoothed density",
xlab="monthly return", ylab="density estimate", col="slateblue1")
qqnorm(return_matrix[,"VBLTX"], col="slateblue1")
qqline(return_matrix[,"VBLTX"])
par(mfrow=c(1,1))
chart.Boxplot(all_returns, names = TRUE, colorset = "slateblue1")
summary(return_matrix)
# A nice PerformanceAnalytics function that computes all of the relevant descriptive statistics is table.Stats
table.Stats(all_returns)
apply(return_matrix, 2, sd)*sqrt(12);
pairs(return_matrix, pch = 16, col = "slateblue1")
cor(return_matrix)
log((29.49)-log(28.64)
)
log(29.49)-log(28.64)
log(109.04)-log(105.06)
exp(43)-1
exp(12*3.7)-1
12*0.02924688
12*0.03718317
10000+10000*.038^12
3.8^12
10000+10000*1.038^12
10000*1.038^12
10000*1.03^12
100000*qnorm(.05, mean = .01, sd = .05)
100000*qnorm(.05, mean = .01, sd = .09)
-7224.268+-13803.68
100000*qnorm(.05, mean = 12*.01, sd = sqrt(12)*.09)
100000*qnorm(.05, mean = 12*.01, sd = sqrt(12)*.05)
swirl()
library(swirl)
swirl()
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmasses, summary)
tapply(flags$population, flags$landmasses, summary)
tapply(flags$population, flags$landmass, summary)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants, 10)
tail(head)
tail(head, 15)
tail(plants, 15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
?sample
sample(1:6, 4, replace = TRUE)
sample(1:6, 4, replace = TRUE)
sample(1:20, 10, replace = FALSE)
sample(1:20, 10)
LETTERS
sample(LETTERS)
flips = sample(c(0,1), 100, replace = TRUE, prob = c(0.3, 0.7))
flips <- sample(c(0,1), 100, replace = TRUE, prob = c(0.3, 0.7))
flips
sum(flips)
?rbinom
rbinom(1, size = 100, prob = 0.7)
flips2 <- rbinom(1, size = 100, prob = 0.7)
flips2 <- rbinom(n = 100, size = 1, prob = 0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(mean = 100, sd = 25, 10)
rpois(5)
rpois(mean = 10, 5)
?rpois
rpois(5, 10)
my_pois <- replicate(100, rpois(5, 10))
my_pois
cm <- colMeans(my_pois)
cm
hist(cm)
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXlt(Sys.time())
class(t2)
t2
unclass(t2)
str(unclass(t2))
t2$min
weekdays(d1)
weekdays(t1)
months(t1)
quarters(t2)
t3 <-  "October 17, 1986 08:24"
t4 <- strptime(t3, "%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(), t1, units = 'days')
swirl()
install_from_swirl("Getting_and_Cleaning_Data")
install_from_swirl("Data_Analysis")
install_from_swirl("Regression_Models")
install_from_swirl("Statistical_Inference")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
install.packages("dplyr")
install.packages("dplyr")
packageVersion("dplyr")
install.packages("foreign")
Sys.setenv(PKG_CPPFLAGS = "-I/usr/local/include/mysql")
Sys.setenv(PKG_LIBS = "-L/usr/local/lib -lmysqlclient")
install.packages("RMySQL", type = "source")
library(RMySQL)
omega(dbReadTable(conn = con,name = 'Test'), title = "9 variables from Thurstone")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(datasets)
data(airquality)
library(ggplot2)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies, smooth = "loess")
qplot
qplot(votes, rating, data = movies) + stats_smooth("loess")
install.packages("knitr")
install.packages("markdown")
prnorm(70, mean = 80, sd = 10)
library(stats)
prnorm(70, mean = 80, sd = 10)
pnorm(70, mean = 80, sd = 10)
pnorm(70, mean = 80, sd = 10, lower.tail = FALSE)
qnorm(p = 95, mean = 1100, sd = 75)
qnorm( = 95, mean = 1100, sd = 75)
qnorm(95, mean = 1100, sd = 75)
dnorm(95, mean = 1100, sd = 75)
qnorm(.95)
qnorm(.95, mean = 1100, sd = 75)
.5^5
.5^4
choose(5,4) * .5^5 + choose(5,5) *.5^5
choose(5,4) + choose(5,5)
.5^4*.5+.5^5
.5^9*.5+.5^10
ppois(10, mean = 5)
ppois(10, lambda = 5)
ppois(10, lambda = 5/3)
ppois(9, lambda = 5)
ppois(9, lambda = 5, lower.tail = FALSE)
pnorm(14, mean = 15, sd = 10)
pnorm(16, mean = 15, sd = 10)
qnorm(.95, mean = 1100, sd = 75/sqrt(100))
pnorm(5, mean = 11, sd = 2)
ppois(9, lambda = 5/3, lower.tail = FALSE)
ppois(10, lambda = 5*3)
ppois(40, lambda = 9*5)
posson.test(x, T= 10)$conf
possoin.test(x, T= 10)$conf
poisson.test(x, T= 10)$conf
poisson.test(10, T= 94.32)$conf
poisson.test(10*60, T= 94.32)$conf
poisson.test(10, T= 60)$conf
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
fit
summary(fit)
str(mtcars)
cars_lm <- lm(data = mtcars, wt~mpg)
summar(cars_lm)
summaru(cars_lm)
summary(cars_lm)
cars_lm
?mtcars
6.0473-2*0.30869
x<- c(1,2,3,4)
x*2
x
predict(cars_lm, newdata = 3000 pounds)
predict(cars_lm, newdata = 3000 )
predict(cars_lm, newdata = data.frame(carat(3000)) )
predict(cars_lm, newdata = data.frame(carat =3000) )
x <- (1000,2000,3000)
x <- c(1000,2000,3000)
predict(cars_lm, newdata = data.frame(carat = x) )
predict(cars_lm, newdata = data.frame(x)
)
mtcarscopy = mtcars
mtcarscopy <- c(1000,2000,3000)
predict(cars_lm, newdata = data.frame(mtcarscopy)
)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y~x)
est<-predict(fit,data.frame(x))
plot(x,y)
abline(fit,col="red")
predict(cars_lm, newdata = data.frame(x = 3000),interval="confidence")
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=mean(x)), interval="confidence")
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=3000), interval="confidence")
fit
summary(fit)
fit1<-lm(mtcars$mpg ~ mtcars$wt)
predict(fit1,data.frame(x=3000), interval="confidence")
summary(fit1)
str(fit1)
fit1$coefficient[1] + 3*fit1$coefficient[2]
fit1$coefficient[1] + 3*fit1$coefficient[2] + 2*0.5591
fit1$coefficient[1] + 2*fit1$coefficient[2]
fit1$coefficient[0] + 2*fit1$coefficient[1]
library(datasets)
data(iris)
str(iris)
sapply(iris, mean)
table(mean(iris$Sepal.Length))
table(mean(iris$Sepal.Length), iris$species)
table(mean(iris$Sepal.Length), iris$Species)
table(iris$Species, mean(iris$Sepal.Length))
sapply(iris, iris$Species, mean)
tapply(iris, iris$Species, mean)
tapply(iris$Sepal.Length, iris$Species, mean)
apply(iris[, 1:4], 2, mean)
data(mtcars)
str(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
sapply(mtcars, cyl, mean)
with(mtcars, tapply(mpg, cyl, mean))
tapply(mpg, cyl, mean)
with(mtcars, tapply(hp, cyl, mean))
mean(mtcars$hp[mtcars$cyl == 8]) - mean(mtcars$hp[mtcars$cyl == 4])
209.21429 -82.63636
debug(ls)
ls
ls()
q
ls
ls()
rm(ls())
file_list <- ls()
rm(file_list)
ls)()
ls()
setwd("/Users/MKarp/datasciencecoursera/RepData/RepData_PeerAssessment2")
storm_data = read.csv("repdata-data-StormData.csv")
plot <- ggplot(storm_data, aes((FATALITIES+INJURIES), EVTYPE) +
geom_point()
plot <- ggplot(storm_data, aes((FATALITIES+INJURIES), EVTYPE)) +
geom_point()
library(ggplot2)
plot <- ggplot(storm_data, aes((FATALITIES+INJURIES), EVTYPE)) +
geom_point()
plot
plot <- ggplot(storm_data, aes(EVTYPE, (FATALITIES+INJURIES))) +
geom_point()
plot
str(storm_data)
summary(storm_data)
plot2 <- ggplot(storm_data, aes(EVTYPE, (CROPDMG+PROPDMG))) +
geom_point()
plot2
plot
library(knitr)
knit(PA2.Rmd)
getwd()
knit("PA2.Rmd")
