class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(), t1, units = 'days')
swirl()
install_from_swirl("Getting_and_Cleaning_Data")
install_from_swirl("Data_Analysis")
install_from_swirl("Regression_Models")
install_from_swirl("Statistical_Inference")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
install.packages("dplyr")
install.packages("dplyr")
packageVersion("dplyr")
install.packages("foreign")
Sys.setenv(PKG_CPPFLAGS = "-I/usr/local/include/mysql")
Sys.setenv(PKG_LIBS = "-L/usr/local/lib -lmysqlclient")
install.packages("RMySQL", type = "source")
library(RMySQL)
omega(dbReadTable(conn = con,name = 'Test'), title = "9 variables from Thurstone")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(datasets)
data(airquality)
library(ggplot2)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies, smooth = "loess")
qplot
qplot(votes, rating, data = movies) + stats_smooth("loess")
install.packages("knitr")
install.packages("markdown")
prnorm(70, mean = 80, sd = 10)
library(stats)
prnorm(70, mean = 80, sd = 10)
pnorm(70, mean = 80, sd = 10)
pnorm(70, mean = 80, sd = 10, lower.tail = FALSE)
qnorm(p = 95, mean = 1100, sd = 75)
qnorm( = 95, mean = 1100, sd = 75)
qnorm(95, mean = 1100, sd = 75)
dnorm(95, mean = 1100, sd = 75)
qnorm(.95)
qnorm(.95, mean = 1100, sd = 75)
.5^5
.5^4
choose(5,4) * .5^5 + choose(5,5) *.5^5
choose(5,4) + choose(5,5)
.5^4*.5+.5^5
.5^9*.5+.5^10
ppois(10, mean = 5)
ppois(10, lambda = 5)
ppois(10, lambda = 5/3)
ppois(9, lambda = 5)
ppois(9, lambda = 5, lower.tail = FALSE)
pnorm(14, mean = 15, sd = 10)
pnorm(16, mean = 15, sd = 10)
qnorm(.95, mean = 1100, sd = 75/sqrt(100))
pnorm(5, mean = 11, sd = 2)
ppois(9, lambda = 5/3, lower.tail = FALSE)
ppois(10, lambda = 5*3)
ppois(40, lambda = 9*5)
posson.test(x, T= 10)$conf
possoin.test(x, T= 10)$conf
poisson.test(x, T= 10)$conf
poisson.test(10, T= 94.32)$conf
poisson.test(10*60, T= 94.32)$conf
poisson.test(10, T= 60)$conf
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
fit
summary(fit)
str(mtcars)
cars_lm <- lm(data = mtcars, wt~mpg)
summar(cars_lm)
summaru(cars_lm)
summary(cars_lm)
cars_lm
?mtcars
6.0473-2*0.30869
x<- c(1,2,3,4)
x*2
x
predict(cars_lm, newdata = 3000 pounds)
predict(cars_lm, newdata = 3000 )
predict(cars_lm, newdata = data.frame(carat(3000)) )
predict(cars_lm, newdata = data.frame(carat =3000) )
x <- (1000,2000,3000)
x <- c(1000,2000,3000)
predict(cars_lm, newdata = data.frame(carat = x) )
predict(cars_lm, newdata = data.frame(x)
)
mtcarscopy = mtcars
mtcarscopy <- c(1000,2000,3000)
predict(cars_lm, newdata = data.frame(mtcarscopy)
)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y~x)
est<-predict(fit,data.frame(x))
plot(x,y)
abline(fit,col="red")
predict(cars_lm, newdata = data.frame(x = 3000),interval="confidence")
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=mean(x)), interval="confidence")
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=3000), interval="confidence")
fit
summary(fit)
fit1<-lm(mtcars$mpg ~ mtcars$wt)
predict(fit1,data.frame(x=3000), interval="confidence")
summary(fit1)
str(fit1)
fit1$coefficient[1] + 3*fit1$coefficient[2]
fit1$coefficient[1] + 3*fit1$coefficient[2] + 2*0.5591
fit1$coefficient[1] + 2*fit1$coefficient[2]
fit1$coefficient[0] + 2*fit1$coefficient[1]
library(datasets)
data(iris)
str(iris)
sapply(iris, mean)
table(mean(iris$Sepal.Length))
table(mean(iris$Sepal.Length), iris$species)
table(mean(iris$Sepal.Length), iris$Species)
table(iris$Species, mean(iris$Sepal.Length))
sapply(iris, iris$Species, mean)
tapply(iris, iris$Species, mean)
tapply(iris$Sepal.Length, iris$Species, mean)
apply(iris[, 1:4], 2, mean)
data(mtcars)
str(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
sapply(mtcars, cyl, mean)
with(mtcars, tapply(mpg, cyl, mean))
tapply(mpg, cyl, mean)
with(mtcars, tapply(hp, cyl, mean))
mean(mtcars$hp[mtcars$cyl == 8]) - mean(mtcars$hp[mtcars$cyl == 4])
209.21429 -82.63636
debug(ls)
ls
ls()
q
?pnorm
probDBP70 <- pnorm(70, mean = 80, sd = 10)
probDBP70
# q3 - Brain volume for adult women is normally distributed with a mean of about 1,100
# cc for women with a standard deviation of 75 cc. What brain volume represents the 95th percentile?
Quant95Brain <- qnorm(.95, mean = 1000, sd = 75)
Quant95Brain
#
Quant95Brain <- 1000+1.96*75
Quant95Brain
Quant95Brain <- qnorm(.95, mean = 1100, sd = 75)
Quant95Brain <- 1100+1.96*75
Quant95Brain <- qnorm(.95, mean = 1100, sd = 75)
Quant95Brain2 <- 1100+1.96*75
Quant95Brain
Quant95Brain2
64^.5
sd1 <- 75
n <- 100
mu <- 1100
mean_sd <- ((sd1^2)/n)^.5
Quant95BrainMu <- qnorm(.95, mean = mu, sd = mean_sd)
Quant95BrainMu
?pbinom
pbinom(c(4,5), size = 5, prob = .5)
pbinom(4, size = 5, prob = .5)
1- pbinom(1, size = 5, prob = .5)
1 - pbinom(1, size = 5, prob = .5)
pbinom(1, size = 5, prob = .5)
morethan4heads <- pbinom(1, size = 5, prob = .5)
morethan4heads
#
Quant95BrainMu
RDI_mean <- pnorm(16, mean = mu, sd = mean_sd) - pnorm(14, mean = mu, sd = mean_sd)
RDI_mean
sd1 <- 10
n <- 100
mu <- 15
mean_sd <- ((sd1^2)/n)^.5
RDI_mean <- pnorm(16, mean = mu, sd = mean_sd) - pnorm(14, mean = mu, sd = mean_sd)
RDI_mean
?dunif
?ppois
bus_mean <- ppois(10, 5)
bus_mean
bus_mean <- ppois(5,10)
bus_mean
ppois(10, 5)
ppois(9, 5)
ppois(5, 9)
ppois(11, 5)
ppois(10, 5)
ppois(9, 5)
bus_mean <- ppois(10,lambda = 5*3)
bus_mean
# 0.06708596
bus_mean <- ppois(9,lambda = 5*3)
bus_mean
# 0.06708596
xy_fit <- lm(y~x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
xy_fit <- lm(y~x)
xy_fit
summary(xy_fit)
mtcars
str(mtcars)
mtcars_reg <- lm(mpg~wt, data = mtcars)
summary(mtcars_reg)
conf95_lower <- 37.2851+(-5.3445-(1.96*.5591))
conf95_lower
-5.3445-(1.96*.5591)
conf95_lower <- 37.2851+(-5.3445*mean(mtcars$wt))
conf95_lower
mean(mtcars$wt)
37.2851+(-(1.96*.5591+5.3445)*mean(mtcars$wt))
predict(mtcars_reg, interval="confidence")
predict(mtcars_reg, data = mean(mtcars$wt) interval="confidence")
predict(mtcars_reg, data = mean(mtcars$wt), interval="confidence")
str(mtcars)
mtcars_reg <- lm(mpg~wt, data = mtcars)
summary(mtcars_reg)
confint(mtcars_reg, 'wt', level=0.95)
confint(mtcars_reg, mean(mtcars$wt), level=0.95)
confint(mtcars_reg, mtcars$wt, level=0.95)
mean(mtcars$wt)
confint(mtcars_reg, 'wt', level=0.95)
2000*-5.3445
sum(c(-4.5432, -2.3647, -0.1252,  1.4096,  6.8727))
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
str(AlzheimerDisease)
AD_df <- data(AlzheimerDisease)
str(AD_df)
AD_df <- AlzheimerDisease
data(AlzheimerDisease)
AlzheimerDisease
?AppliedPredictiveModeling
AppliedPredictiveModeling
AppliedPredictiveModeling
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
str(adData)
str(training)
str(testing)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
str(training)
str(testing)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
install.packages("Hmisc")
library(Hmisc)
hist(mixtures$SuperPlasticizer)
str(mixtures)
hist(mixtures$Superplasticizer)
summary(mixtures)
hist(log(mixtures$Superplasticizer))
log(0)
hist(sqroot(mixtures$Superplasticizer))
hist(sqrt(mixtures$Superplasticizer))
plot(mixtures$CompressiveStrength, nrow(mixtures))
plot(1:nrow(mixtures), mixtures$CompressiveStrength)
plot(1:nrow(mixtures), mixtures$CompressiveStrength, col = as.factor(FlyAsh))
plot(1:nrow(mixtures), mixtures$CompressiveStrength, col = as.factor(mixtures$FlyAsh))
plot(1:nrow(mixtures), mixtures$CompressiveStrength, col = as.factor(mixtures$Age))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
?cut2
library(caret)
library(Hmisc)
?cut2
library(ggplot2)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(mixtures)
cutCement <- cut2(training$Cement, g = 3)
cutCement <- cut2(training$Cement, g = 4)
cutFlyAsh <- cut2(training$FlyAsh, g = 4)
cutAge <- cut2(training$Age, g = 4)
qplot(training$CompressiveStrength, training$FlyAsh, colour = training$Age)
qplot(training$CompressiveStrength, training$Age, colour = training$FlyAsh)
qplot(training$CompressiveStrength, 1:nrow(training), colour = training$FlyAsh)
qplot(training$CompressiveStrength, 1:nrow(training), colour = training$Age)
qplot(training$CompressiveStrength, 1:nrow(training), colour = training$FlyAsh)
library(knit)
library(knitr)
new_mean <- 3
new_var <- .6
old_mean <- 5
old_var <- .68
sp <- (9)*+(9)(.68)^2
total_df <- 18
standard error <- sqrt(((.6)^2)/9)+((.68)^2)/9))
num <- (.6^2/10 + .68^2/10)^2
den <- .6^4/10^2/9 + .68^4/10^2/9
mydf <- num/den
conf95 <- new_mean-old_mean+c(-1,1)*qt(.975,mydf)*sqrt(num)
conf95
se <- sqrt(new_var*new_var/10+old_var*old_var/10)
se
new_mean <- 3
old_mean <- 5
conf95 <- new_mean-old_mean+c(-1,1)*qt(.975,mydf)*se
conf95
n <- 10000; means <- cumsum(rnorm(n)) / (1  : n)
plot(1 : n, means, type = "l", lwd = 2,
frame = FALSE, ylab = "cumulative means", xlab = "sample size")
abline(h = 0)
?rexp
rexp(40,.2)
rexp(1000,.2)
mean(rexp(1000,.2))
mean(rexp(10000,.2))
mean(rexp(10000,1/.2))
1/.2
rexp(40,1/.2)
mean(rexp(40,1/.2))
mean(rexp(50,1/.2))
mean(rexp(60,1/.2))
mean(rexp(70,1/.2))
mean(rexp(80,1/.2))
hist(runif(1000))
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(runif(40)))
hist(mns)
par(mfrow = c(2, 3))
for (n in c(1, 10, 20)){
temp <- matrix(sample(0 : 1, n * 10000, replace = TRUE, prob = c(.9, .1)), ncol = n)
temp <- apply(temp, 1, mean)
temp <- (temp - .1) / sqrt(.1 * .9 / n)
dty <- density(temp)
plot(dty$x, dty$y, xlab = "", ylab = "density", type = "n", xlim = c(-3, 3), ylim = c(0, .5))
title(paste("sample mean of", n, "obs"))
lines(seq(-3, 3, length = 100), dnorm(seq(-3, 3, length = 100)), col = grey(.8), lwd = 3)
lines(dty$x, dty$y, lwd = 2)
}
for (n in c(1, 10, 20)){
temp <- matrix(sample(0 : 1, n * 10000, replace = TRUE), ncol = n)
temp <- apply(temp, 1, mean)
temp <- (temp - .5) * 2 * sqrt(n)
dty <- density(temp)
plot(dty$x, dty$y, xlab = "", ylab = "density", type = "n", xlim = c(-3, 3), ylim = c(0, .5))
title(paste("sample mean of", n, "obs"))
lines(seq(-3, 3, length = 100), dnorm(seq(-3, 3, length = 100)), col = grey(.8), lwd = 3)
lines(dty$x, dty$y, lwd = 2)
lamdba <- 0.2
n <- 40
hist(runif(rexp(n, 1/lambda)))
lamdba <- 0.2
n <- 40
hist(runif(rexp(n, 1/lambda)))
lambda <- 0.2
n <- 40
hist(runif(rexp(n, 1/lambda)))
par(mfrow = c(1, 1))
lambda <- 0.2
n <- 40
hist(runif(rexp(n, 1/lambda)))
# run a hisotgram of a sample of this type of dustribution
hist(runif(rexp(n, 1/lambda)))
theo_mean <- 1/lambda
?sample
mns = (sample(x = mean(rexp(n, 1/lambda)), size = 1000))
mns = (sample(x = mean(rexp(n, 1/lambda)), size = 1000, replace = TRUE))
mns
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(rexp(n, 1/lambda)))
mean(mns)
hist(mns)
theo_mean
hist(runif(rexp(n, lambda)))
mns = NULL
# loop 1000 times to get 1000 simulations
for (i in 1 : 1000) mns = c(mns, mean(rexp(n, lambda)))
theo_mean
mean(mns)
sample_mean1000 <- mean(mns)
set.seed(1000)
an exponential
# distribution
lambda <- 0.2
# setting n = 40 as set by the spec
n <- 40
# run a hisotgram of a sample of this type of dustribution
hist(runif(rexp(n, lambda)))
# set theoretical mean for comparison
theo_mean <- 1/lambda
# mean samples variable
mns = NULL
# loop 1000 times to get 1000 simulations
for (i in 1 : 1000) mns = c(mns, mean(rexp(n, lambda)))
hist(mns)
# 1 Show the sample mean and compare it to the theoretical mean of the distribution.
theo_mean
# theoretical mean = 1/lambda = 1/.2 = 5
# mean of sample means after running 1000 simulations of the exponential rexp(40, .2)
sample_mean1000 <- mean(mns)
sample_mean1000
# 5.008442
hist(rexp(n, lambda)))
hist(rexp(n, lambda))
theo_sample_diff <- sample_mean1000-theo_mean
theo_sample_diff
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
str(sampleTimes)
adate <- as.date( "2007-01-03")
adate <- as.Date( "2007-01-03")
str(adate)
amzn2012 <- sampleTimes["2012" %in% sampleTimes]
str(amz2012)
str(amzn2012)
table(as.Date(sampleTimes) >= as.Date("2012-01-01") & as.Date(sampleTimes) <= as.Date("2012-12-31"))
weekdays(adate)
table(as.Date(sampleTimes) >= as.Date("2012-01-01") & as.Date(sampleTimes) <= as.Date("2012-12-31") & weekdays(as.Date(sampleTimes) == "Monday"))
table(as.Date(sampleTimes) >= as.Date("2012-01-01") & as.Date(sampleTimes) <= as.Date("2012-12-31") & weekdays(as.Date(sampleTimes))== "Monday"))
table(as.Date(sampleTimes) >= as.Date("2012-01-01") & as.Date(sampleTimes) <= as.Date("2012-12-31") & weekdays(as.Date(sampleTimes)) == "Monday")
setwd("~/datasciencecoursera/Getting and Cleaning Data")
dat <- read.csv("getdata-data-GDP.csv")
str(dat)
summary(dat)
str(dat$Gross.domestic.product.2012)
dat$Gross.domestic.product.2012
summary(dat)
str(dat)
dat$X.3
?as.numeric
mean(as.numeric(dat$X.3), na.rm = TRUE)
gsub(",", "", "1,234,567", fixed = TRUE)
gsub(",", "", dat$X.3, fixed = TRUE)
mean(as.numeric(gsub(",", "", dat$X.3, fixed = TRUE) ), na.rm = TRUE)
gdp <- gsub(",", "", dat$X.3, fixed = TRUE)
gdp
mean(as.numeric(gdp), na.rm = TRUE)
mean(as.numeric(gdp))
mean(as.numeric(gdp), na.rm = TRUE)
?ttest
x <- c(140, 138, 150, 148, 135)
y <- c(132, 135,151,146,130)
t.test(y~x)
t.test(y, mu = mean(x))
t.test(x, mu = mean(y))
?pt
t.test(x, y, alternative = c("two.sided"), mu = 0)
t.test(y~x, alternative = c("two.sided"), mu = 0)
t.test(x, y, alternative = c("less"), mu = 0)
t.test(x, y, alternative = c("less"))
t.test(y, x, alternative = c("less"))
