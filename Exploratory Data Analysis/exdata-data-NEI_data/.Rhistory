# Split plotting window in three rows
par(mfrow=c(3,1))
# First plot: The simulated observations
plot(ma1_sim, type="l",main="MA(1) Process: mu=0.05, theta=0.9",xlab="time",ylab="y(t)")
abline(h=0)
# Second plot: Theoretical ACF
plot(1:10, acf_ma1_model[2:11], type="h", col="blue", main="theoretical ACF")
# Third plot: Sample ACF
tmp=acf(ma1_sim, lag.max=10, main="Sample ACF")
# Reset graphical window to only one graph
par(mfrow=c(1,1))
#tests for graphs
set.seed(123);
# Simulate 250 observations from the described AR(1) model
ar1_sim = arima.sim(model=list(ar=0.9), n=250, mean=0, sd=0.1) + 0.05;
# Generate the theoretical ACF with ten lags
acf_ar1_model = ARMAacf(ar=0.9, lag.max=10)
# Split plotting window in three rows
par(mfrow=c(3,1))
# Generate the same three graphs as in the previous exercise
par(mfrow=c(3,1))
# First plot: The simulated observations
plot(ar1_sim, type="l", main="AR(1) Process: mu=0.05, phi=0.9",xlab="time",ylab="y(t)")
abline(h=0)
# Second plot: Theoretical AFC
plot(1:10, acf_ar1_model[2:11], type="h", col="blue", main="theoretical ACF")
# Third plot: Sample AFC
tmp = acf(ar1_sim, lag.max=10, main="Sample ACF")
# Reset plotting window to default
par(mfrow=c(1,1));
install.packages(PerformanceAnalytics)
install.packages("PerformanceAnalytics")
install.packages("zoo")
install.packages("tseries")
library(PerformanceAnalytics)
library(zoo)
library(tseries)
?get.hist.quote
# getting data into time series objects from yahoo
# Load relevant packages
library(PerformanceAnalytics);library(zoo);library(tseries);
# Get the monthly adjusted closing price data on VBLTX, FMAGX and SBUX from Yahoo! using the tseries function get.hist.quote(). Set the sample to Jan 1998 through Dec 2009.
# Get the adjusted closing prices from Yahoo!
VBLTX_prices = get.hist.quote(instrument="vbltx", start="1998-01-01",end="2009-12-31", quote="AdjClose",provider="yahoo", origin="1970-01-01",compression="m", retclass="zoo")
FMAGX_prices = get.hist.quote(instrument="fmagx", start="1998-01-01",end="2009-12-31", quote="AdjClose",provider="yahoo", origin="1970-01-01",compression="m", retclass="zoo")
SBUX_prices = get.hist.quote(instrument="sbux", start="1998-01-01",end="2009-12-31", quote="AdjClose",provider="yahoo", origin="1970-01-01",compression="m", retclass="zoo")
# Change the class of the time index to yearmon which is appropriate for monthly data
# index() and as.yearmon() are functions in the zoo package
index(VBLTX_prices) = as.yearmon(index(VBLTX_prices))
index(FMAGX_prices)  = as.yearmon(index(FMAGX_prices))
index(SBUX_prices)  = as.yearmon(index(SBUX_prices))
# Inspect your data
start(SBUX_prices)
end(SBUX_prices)
# The variables VBLTX_prices, FMAGX_prices and SBUX_prices are preloaded in your workspace
# Create merged price data
all_prices = merge(VBLTX_prices, FMAGX_prices, SBUX_prices)
# Rename columns
colnames(all_prices) = c("VBLTX", "FMAGX", "SBUX")
# Calculate cc returns as difference in log prices
all_returns = diff(log(all_prices))
# Look at the return data
start(all_returns)
end(all_returns)
colnames(all_returns)
head(all_returns)
# Generate a chart based on assets in portfolio displaying cumulative return on 1$ since purchase
# 'all_returns' is preloaded in your workspace.
# Plot returns after using the PerformanceAnalytics function chart.TimeSeries().
# This function creates a slightly nicer looking plot than plot.zoo()
chart.TimeSeries(all_returns, legend.loc="bottom", main=" ")
# The previous charts are a bit hard to read. The PerformanceAnalytics function
# chart.Bar makes it easier to compare the returns of different assets on the
# same plot
chart.Bar(all_returns, legend.loc="bottom", main=" ")
# Cumulative return plot - must use simple returns (!) and not cc returns for this
# Use PerformanceAnalytics function chart.CumReturns()
simple_returns = diff(all_prices)/lag(all_prices, k=-1);
chart.CumReturns(simple_returns, wealth.index = TRUE, legend.loc = "topleft", main = "Future Value of $1 invested")
# Create matrix with returns
return_matrix = coredata(all_returns);
# Generate four panel plots
par(mfrow=c(2,2))
hist(return_matrix[,"VBLTX"],main="VBLTX monthly returns",
xlab="VBLTX", probability=T, col="slateblue1")
boxplot(return_matrix[,"VBLTX"],outchar=T, main="Boxplot", col="slateblue1")
plot(density(return_matrix[,"VBLTX"]),type="l", main="Smoothed density",
xlab="monthly return", ylab="density estimate", col="slateblue1")
qqnorm(return_matrix[,"VBLTX"], col="slateblue1")
qqline(return_matrix[,"VBLTX"])
par(mfrow=c(1,1))
chart.CumReturns(simple_returns, wealth.index = TRUE, legend.loc = "topleft", main = "Future Value of $1 invested")
# Generate four panel plots
par(mfrow=c(2,2))
hist(return_matrix[,"VBLTX"],main="VBLTX monthly returns",
xlab="VBLTX", probability=T, col="slateblue1")
boxplot(return_matrix[,"VBLTX"],outchar=T, main="Boxplot", col="slateblue1")
plot(density(return_matrix[,"VBLTX"]),type="l", main="Smoothed density",
xlab="monthly return", ylab="density estimate", col="slateblue1")
qqnorm(return_matrix[,"VBLTX"], col="slateblue1")
qqline(return_matrix[,"VBLTX"])
par(mfrow=c(1,1))
chart.Boxplot(all_returns, names = TRUE, colorset = "slateblue1")
summary(return_matrix)
# A nice PerformanceAnalytics function that computes all of the relevant descriptive statistics is table.Stats
table.Stats(all_returns)
apply(return_matrix, 2, sd)*sqrt(12);
pairs(return_matrix, pch = 16, col = "slateblue1")
cor(return_matrix)
log((29.49)-log(28.64)
)
log(29.49)-log(28.64)
log(109.04)-log(105.06)
exp(43)-1
exp(12*3.7)-1
12*0.02924688
12*0.03718317
10000+10000*.038^12
3.8^12
10000+10000*1.038^12
10000*1.038^12
10000*1.03^12
100000*qnorm(.05, mean = .01, sd = .05)
100000*qnorm(.05, mean = .01, sd = .09)
-7224.268+-13803.68
100000*qnorm(.05, mean = 12*.01, sd = sqrt(12)*.09)
100000*qnorm(.05, mean = 12*.01, sd = sqrt(12)*.05)
swirl()
library(swirl)
swirl()
sapply(flags, unique)
vapply(flags, unique, numeric(1))
ok()
sapply(flags, class)
vapply(flags, class, character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population, flags$red, summary)
tapply(flags$population, flags$landmasses, summary)
tapply(flags$population, flags$landmasses, summary)
tapply(flags$population, flags$landmass, summary)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants, 10)
tail(head)
tail(head, 15)
tail(plants, 15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
?sample
sample(1:6, 4, replace = TRUE)
sample(1:6, 4, replace = TRUE)
sample(1:20, 10, replace = FALSE)
sample(1:20, 10)
LETTERS
sample(LETTERS)
flips = sample(c(0,1), 100, replace = TRUE, prob = c(0.3, 0.7))
flips <- sample(c(0,1), 100, replace = TRUE, prob = c(0.3, 0.7))
flips
sum(flips)
?rbinom
rbinom(1, size = 100, prob = 0.7)
flips2 <- rbinom(1, size = 100, prob = 0.7)
flips2 <- rbinom(n = 100, size = 1, prob = 0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(mean = 100, sd = 25, 10)
rpois(5)
rpois(mean = 10, 5)
?rpois
rpois(5, 10)
my_pois <- replicate(100, rpois(5, 10))
my_pois
cm <- colMeans(my_pois)
cm
hist(cm)
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXlt(Sys.time())
class(t2)
t2
unclass(t2)
str(unclass(t2))
t2$min
weekdays(d1)
weekdays(t1)
months(t1)
quarters(t2)
t3 <-  "October 17, 1986 08:24"
t4 <- strptime(t3, "%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(), t1, units = 'days')
swirl()
install_from_swirl("Getting_and_Cleaning_Data")
install_from_swirl("Data_Analysis")
install_from_swirl("Regression_Models")
install_from_swirl("Statistical_Inference")
swirl()
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
install.packages("dplyr")
install.packages("dplyr")
packageVersion("dplyr")
install.packages("foreign")
Sys.setenv(PKG_CPPFLAGS = "-I/usr/local/include/mysql")
Sys.setenv(PKG_LIBS = "-L/usr/local/lib -lmysqlclient")
install.packages("RMySQL", type = "source")
library(RMySQL)
omega(dbReadTable(conn = con,name = 'Test'), title = "9 variables from Thurstone")
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
library(datasets)
data(airquality)
library(ggplot2)
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
qplot(Wind, Ozone, data = airquality, geom = "smooth")
library(datasets)
data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
g <- ggplot(movies, aes(votes, rating))
print(g)
qplot(votes, rating, data = movies)
qplot
qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies, smooth = "loess")
qplot
qplot(votes, rating, data = movies) + stats_smooth("loess")
install.packages("knitr")
install.packages("markdown")
prnorm(70, mean = 80, sd = 10)
library(stats)
prnorm(70, mean = 80, sd = 10)
pnorm(70, mean = 80, sd = 10)
pnorm(70, mean = 80, sd = 10, lower.tail = FALSE)
qnorm(p = 95, mean = 1100, sd = 75)
qnorm( = 95, mean = 1100, sd = 75)
qnorm(95, mean = 1100, sd = 75)
dnorm(95, mean = 1100, sd = 75)
qnorm(.95)
qnorm(.95, mean = 1100, sd = 75)
.5^5
.5^4
choose(5,4) * .5^5 + choose(5,5) *.5^5
choose(5,4) + choose(5,5)
.5^4*.5+.5^5
.5^9*.5+.5^10
ppois(10, mean = 5)
ppois(10, lambda = 5)
ppois(10, lambda = 5/3)
ppois(9, lambda = 5)
ppois(9, lambda = 5, lower.tail = FALSE)
pnorm(14, mean = 15, sd = 10)
pnorm(16, mean = 15, sd = 10)
qnorm(.95, mean = 1100, sd = 75/sqrt(100))
pnorm(5, mean = 11, sd = 2)
ppois(9, lambda = 5/3, lower.tail = FALSE)
ppois(10, lambda = 5*3)
ppois(40, lambda = 9*5)
posson.test(x, T= 10)$conf
possoin.test(x, T= 10)$conf
poisson.test(x, T= 10)$conf
poisson.test(10, T= 94.32)$conf
poisson.test(10*60, T= 94.32)$conf
poisson.test(10, T= 60)$conf
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
fit
summary(fit)
str(mtcars)
cars_lm <- lm(data = mtcars, wt~mpg)
summar(cars_lm)
summaru(cars_lm)
summary(cars_lm)
cars_lm
?mtcars
6.0473-2*0.30869
x<- c(1,2,3,4)
x*2
x
predict(cars_lm, newdata = 3000 pounds)
predict(cars_lm, newdata = 3000 )
predict(cars_lm, newdata = data.frame(carat(3000)) )
predict(cars_lm, newdata = data.frame(carat =3000) )
x <- (1000,2000,3000)
x <- c(1000,2000,3000)
predict(cars_lm, newdata = data.frame(carat = x) )
predict(cars_lm, newdata = data.frame(x)
)
mtcarscopy = mtcars
mtcarscopy <- c(1000,2000,3000)
predict(cars_lm, newdata = data.frame(mtcarscopy)
)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit<-lm(y~x)
est<-predict(fit,data.frame(x))
plot(x,y)
abline(fit,col="red")
predict(cars_lm, newdata = data.frame(x = 3000),interval="confidence")
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=mean(x)), interval="confidence")
x<-mtcars$wt
y<-mtcars$mpg
fit<-lm(y ~ x)
predict(fit,data.frame(x=3000), interval="confidence")
fit
summary(fit)
fit1<-lm(mtcars$mpg ~ mtcars$wt)
predict(fit1,data.frame(x=3000), interval="confidence")
summary(fit1)
str(fit1)
fit1$coefficient[1] + 3*fit1$coefficient[2]
fit1$coefficient[1] + 3*fit1$coefficient[2] + 2*0.5591
fit1$coefficient[1] + 2*fit1$coefficient[2]
fit1$coefficient[0] + 2*fit1$coefficient[1]
library(datasets)
data(iris)
str(iris)
sapply(iris, mean)
table(mean(iris$Sepal.Length))
table(mean(iris$Sepal.Length), iris$species)
table(mean(iris$Sepal.Length), iris$Species)
table(iris$Species, mean(iris$Sepal.Length))
sapply(iris, iris$Species, mean)
tapply(iris, iris$Species, mean)
tapply(iris$Sepal.Length, iris$Species, mean)
apply(iris[, 1:4], 2, mean)
data(mtcars)
str(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
sapply(mtcars, cyl, mean)
with(mtcars, tapply(mpg, cyl, mean))
tapply(mpg, cyl, mean)
with(mtcars, tapply(hp, cyl, mean))
mean(mtcars$hp[mtcars$cyl == 8]) - mean(mtcars$hp[mtcars$cyl == 4])
209.21429 -82.63636
debug(ls)
ls
ls()
q
setwd("~/Documents/Computer:Data Science/R")
# final round
final = read.csv("std_analyst_final.csv",stringsAsFactors=FALSE)
str(final)
summary(final)
final$analyst= as.factor(final$analyst)
final$estimator = as.factor(final$estimator)
final$date.estimate = as.Date(final$date.estimate, "%m/%d/%y")
final$date.actual = as.Date(final$date.actual, "%m/%d/%y")
final$ticker = as.factor(final$ticker)
final$percent.error = as.numeric(final$percent.error)
str(final)
#install.pac
library(caTools)
set.seed(123)
# Split data set based on 3rd quartile
table(final$date.actual < "2012-01-04")
train = subset(final, date.actual < "2011-09-04")
test = subset(final, date.actual >= "2011-09-04")
str(train)
str(test)
# Ensure analyst examples are in both tain and test sets
train = subset(train, analyst %in% test$analyst & estimator %in% test$estimator & ticker %in% test$ticker)
test = subset(test, analyst %in% train$analyst & estimator %in% train$estimator & ticker %in% train$ticker)
train = subset(train, analyst %in% test$analyst & estimator %in% test$estimator & ticker %in% test$ticker)
test = subset(test, analyst %in% train$analyst & estimator %in% train$estimator & ticker %in% train$ticker)
#train = subset(train, estimator %in% test$estimator)
#test = subset(test.lm, estimator %in% train.lm$estimator)
#train.lm2 = subset(train.lm1, ticker %in% test.lm1$ticker)
#test.lm2 = subset(test.lm1, ticker %in% train.lm1$ticker)
#train.lm2 = subset(train.lm1, ticker %in% test.lm2$ticker)
#test.lm2 = subset(test.lm2, ticker %in% train.lm2$ticker)
train = subset(train, !is.na(percent.error))
test = subset(test, !is.na(percent.error))
str(train)
str(test)
# EPS Error ~
# mean eps error = -0.005
SST_Error = sum((mean(train$error) - test$error)^2)
# 4900.772
# Eps Error ~
# Analyst code
# Estimator Code
# Date difference
# Ticker
# Regression w/o analyst info stdzd
PE_Reg = lm(formula = error ~ ticker+estimator+stnd.date.difference, data = train)
# mult r2 = 0.1501
#Predictions
PE_Reg_Predict = predict(PE_Reg, newdata = test)
PE_REG_SSE = sum((PE_Reg_Predict  - test$error)^2)
PE_REG_SSE
# 6846.78
PE_REG_R2 = 1 - PE_REG_SSE/SST_Error
PE_REG_R2
# -0.4
PE_RMSE = sqrt(PE_REG_SSE/nrow(test))
PE_RMSE
# 0.46
# Regression w/ analyst info stdzd
PE_Reg_A = lm(formula = error ~ analyst+ticker+estimator+stnd.date.difference, data = train)
# mult r2 = 0.1501
#Predictions
PE_Reg_Predicta = predict(PE_Reg_A, newdata = test)
PE_REG_SSEa = sum((PE_Reg_Predicta  - test$error)^2)
PE_REG_SSEa
# 6992.208
PE_REG_R2a = 1 - PE_REG_SSEa/SST_Error
PE_REG_R2a
# -0.43
PE_RMSEa = sqrt(PE_REG_SSEa/nrow(test))
PE_RMSEa
# 0.47
#kNN
library(kknn)
# Regression w/o analyst info stdzd
E_knn = kknn(formula = error ~ ticker+estimator+stnd.date.difference, data = train, k = 5)
# mult r2 = 0.1501
#Predictions
PE_Reg_Predict = predict(PE_Reg, newdata = test)
PE_REG_SSE = sum((PE_Reg_Predict  - test$error)^2)
PE_REG_SSE
# 6846.78
PE_REG_R2 = 1 - PE_REG_SSE/SST_Error
PE_REG_R2
# -0.4
PE_RMSE = sqrt(PE_REG_SSE/nrow(test))
PE_RMSE
# 0.46
# Regression w/ analyst info stdzd
PE_Reg_A = lm(formula = error ~ analyst+ticker+estimator+stnd.date.difference, data = train)
# mult r2 = 0.1501
#Predictions
PE_Reg_Predicta = predict(PE_Reg_A, newdata = test)
PE_REG_SSEa = sum((PE_Reg_Predicta  - test$error)^2)
PE_REG_SSEa
# 6992.208
PE_REG_R2a = 1 - PE_REG_SSEa/SST_Error
PE_REG_R2a
# -0.43
PE_RMSEa = sqrt(PE_REG_SSEa/nrow(test))
PE_RMSEa
# 0.47
E_matrix_train = model.matrix(error ~ ticker+estimator+stnd.date.difference, data = train)
EA_matrix_train = model.matrix(error ~ analyst+ticker+estimator+stnd.date.difference, data = train)
E_matrix_test = model.matrix(error ~ ticker+estimator+stnd.date.difference, data = test)
EA_matrix_test = model.matrix(error ~ analyst+ticker+estimator+stnd.date.difference, data = test)
# Create a CART model w/o analyst info stdzd
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)
PE_tree = rpart(formula = error ~ ticker+estimator+stnd.date.difference+stnd.company.age+stnd.market.cap+stnd.beta+stnd.oil.index+stnd.gold.index+stnd.unemployment, data = train)
PE_tree = rpart(formula = error ~ ticker+estimator+stnd.date.difference, data = train)
prp(PE_tree)
summary(PE_tree)
plot(PE_tree)
setwd("~/datasciencecoursera/Exploratory Data Analysis")
# MKarp94
# Coursera - Exploratory Data Analysis - Data Science Specialization
# Proj2 - plot6.R
setwd("/Users/MKarp/datasciencecoursera/Exploratory Data Analysis/exdata-data-NEI_data")
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")
NEI$year <- factor(NEI$year)
NEI$type <- factor(NEI$type)
library(ggplot2)
balt.la.NEI <- NEI[(NEI$fips == "24510" | NEI$fips == "06037") & NEI$type == "ON-ROAD",]
plot <- ggplot(balt.la.NEI, aes(year, Emissions)) +
geom_point() + facet_grid(. ~ fips)
ggsave(file="plot6.png")
plot
