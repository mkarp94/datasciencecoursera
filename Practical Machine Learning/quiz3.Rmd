---
author: "Michael Karp"
title: " Practical Machine Learning - Quiz3 - Data Science Specialization"
date: 1/22/2015
output: 
html_document:
keep_md: true
---

```{r, echo = TRUE, eval=TRUE}
# q1 - Load the cell segmentation data from the AppliedPredictiveModeling package using the commands:
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set. 
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]; testing <- segmentationOriginal[-inTrain,]
dim(training); dim(testing)
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings. 
set.seed(125)
# fit classification tree to segmentation data
library(rpart)
segment_cart <- rpart(data=training, formula = Class~., method="class")
# print tree
# printcp(segment_cart)
# lotcp(segment_cart)
library(rpart.plot)
prp(segment_cart)
# summary(segment_cart)
```


```{r, dependson="createTree", fig.height=4.5, fig.width=4.5}
library(rattle)
fancyRpartPlot(segment_cart)
# 3. In the final model what would be the final model prediction for cases with the following variable values:
#  a. TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2   PS
#  b. TotalIntench2 = 50,000; FiberWidthCh1 = 10;VarIntenCh4 = 100   WS
#  c. TotalIntench2 = 57,000; FiberWidthCh1 = 8;VarIntenCh4 = 100    PS
#  d. FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2          No way to tell
 
# q2 - If K is small in a K-fold cross validation is the bias in the estimate of out-of-sample (test set) accuracy
# smaller or bigger? If K is small is the variance in the estimate of out-of-sample (test set) accuracy smaller or
# bigger. Is K large or small in leave one out cross validation? The bias is larger and the variance is smaller. 
# Under leave one out cross validation K is equal to the sample size.
``` 

```{r, echo = TRUE, eval=TRUE}
# q3 - Load the olive oil data using the commands:
library(pgmm)
data(olive)
olive = olive[,-1]
# Fit a classification  tree where Area is the outcome variable. Then predict the value of area for the following 
# data frame using the tree command with all defaults
olive_cart <- rpart(data= olive, formula = Area~.)
newdata = as.data.frame(t(colMeans(olive)))
# What is the resulting prediction? Is the resulting prediction strange? Why or why not?
olive_pred <- predict(olive_cart, newdata = newdata)
olive_pred
````
```{r, echo = TRUE, eval=TRUE}
# q4 - Load the South Africa Heart Disease Data and create training and test sets with the following code:
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
# Then set the seed to 13234 and fit a logistic regression model (method="glm", be sure to specify family="binomial")
# with Coronary Heart Disease (chd) as the outcome and age at onset, current alcohol consumption, obesity levels,
# cumulative tabacco, type-A behavior, and low density lipoprotein cholesterol as predictors. Calculate the 
# misclassification rate for your model using this function and a prediction on the "response" scale:
# missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
# What is the misclassification rate on the training set? What is the misclassification rate on the test set?
set.seed(13234)
str(SAheart)
saheart_logistic <- train(data = trainSA, chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
train_misclass <- missClass(trainSA$chd, predict(saheart_logistic, trainSA))
train_misclass
test_misclass <- missClass(testSA$chd, predict(saheart_logistic, testSA))
test_misclass
```
```{r, echo = TRUE, eval=TRUE}
# q5 - Load the vowel.train and vowel.test data sets:
library(ElemStatLearn)
data(vowel.train)
data(vowel.test) 
# Set the variable y to be a factor variable in both the training and test set. Then set the seed to 33833. Fit a 
# random forest predictor relating the factor variable y to the remaining variables. Read about variable importance in 
# random forests here: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr The caret package uses by 
# defualt the Gini importance. Calculate the variable importance using the varImp function in the caret package. What 
# is the order of variable importance?
set.seed(33833)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
str(vowel.train)
library(randomForest)
vowel_forest <- randomForest(data = vowel.train, y~.)
varImp(vowel_forest)
varImpPlot(vowel_forest)
```